[{"authors":["Xiaojun Chang"],"categories":null,"content":"Dr Xiaojun Chang joined UTS as a Professor in the Australian Artificial Intelligence Institute (AAII) in 2022, bringing his globally renowned expertise in artificial intelligence, computer vision, multimedia, neural networks and machine learning. He is also a Visiting Professor in the Department of Computer Vision at Mohamed bin Zayed University of Artificial Intelligence (MBZUAI).\nAt AAII, he is the Director of the Recognition, Learning and Reasoning Lab (ReLER) where he explores multiple signals (visual, acoustic, textual) for automatic content analysis in unconstrained or surveillance videos.\nPrior to this, Xiaojun received his PhD in Computer Science from UTS in 2016 before moving to Carnegie Mellon University in the U.S. as a postdoctoral research associate. After being awarded an ARC DECRA in 2018, Xiaojun returned to Australia to take up a lecturer position at Monash University. He was promoted to Senior Lecturer in 2020, then joined RMIT University as an Associate Professor in 2021.\nWith a focus on real-world outcomes, Xiaojun has secured over $3 million in research funding and has made significant contributions to the field of video analysis and multimedia information retrieval, including for health care and management.\nXiaojun was named a Clarivate Analytics Highly Cited Researcher in 2019, 2020, 2021 and 2023, and his work has been covered by media from Australia, US, Europe and China, including his ground-breaking work developing an automatic report generation system for critically ill COVID-19 patients using deep learning techniques.\nHis team has won multiple prizes from international grand challenges, hosting competitive teams from MIT, University of Maryland, Facebook AI Research (FAIR) and Baidu VIS. He also won first prize in TrecVID 2019 - Activity Extended Video (ActEV) challenge, held by the National Institute of Standards and Technology in the U.S.\nXiaojun engages regularly with industry and has published a total of over 200 peer reviewed research papers and top conference papers. He has also served as an area chair for many prestigious international conferences, such as ACM Multimedia 2019, 2020, 2021 and 2022, IJCAI 2018, and ICPR 2018. Xiaojun has reviewed ARC Discovery Grant applications and served as an Associate Editor and Guest Editor on multiple prestigious journals.\nIncredibly passionate about his work, Xiaojun’s ultimate aim is to advance the field of artificial intelligence to better serve humanity by creating more accurate and efficient computational models that can understand and interact with the real world in a meaningful way.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1ad3e3034dbebe1dd503bab579e0a180","permalink":"/author/xiaojun-chang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaojun-chang/","section":"authors","summary":"Dr Xiaojun Chang joined UTS as a Professor in the Australian Artificial Intelligence Institute (AAII) in 2022, bringing his globally renowned expertise in artificial intelligence, computer vision, multimedia, neural networks and machine learning.","tags":null,"title":"Xiaojun Chang","type":"authors"},{"authors":["Xun Yang"],"categories":null,"content":"Xun Yang now is a Professor in Department of Electronic Engineering and Information Science (EEIS), University of Science and Technology of China (USTC), Hefei, China. He was a research fellow in NExT++, School of Computing, National University of Singapore from 2018 to 2021. Xun received his Ph.D from Hefei University of Technology in 2017, under the supervision of Prof. Meng Wang. His research interests include information retrieval, computer vision, and multimedia information processing. Moreover, he has served as the PC member and the invited reviewer for top-tier conferences and prestigious journals including ACM MM, ACM MMAsia, AAAI, IJCAI, IEEE TCSVT, IEEE TKDE, IEEE TNNLS, and IEEE TMM.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"21cabdfd742f09b06732317c7c93ad3c","permalink":"/author/xun-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xun-yang/","section":"authors","summary":"Xun Yang now is a Professor in Department of Electronic Engineering and Information Science (EEIS), University of Science and Technology of China (USTC), Hefei, China. He was a research fellow in NExT++, School of Computing, National University of Singapore from 2018 to 2021.","tags":null,"title":"Xun Yang","type":"authors"},{"authors":["Zhihui Li"],"categories":null,"content":"Greetings! I received my PhD degree from School of Computer Science and Engineering, University of New South Wales (UNSW), Australia. My supervisors are A/Professor Lina Yao and Professor Salil Kanhere. Before joining UNSW, I have worked as a research assistant with School of Computer Science and Technology at Shandong University. I received the B.S. degree from Beijing University of Posts and Telecommunications in 2008. My research interests include artificial intelligence, machine learning, and computer vision. I have published about 20 technical papers at prominent journals and conferences such as IEEE Transactions on Knowledge and Data Engineering (TKDE), IEEE Transactions on Neural Networks and Learning Systems (TNNLS), IEEE Transactions on Cybernetics (TCYB), IEEE Transactions on Image Processing (T-IP), IEEE Transactions on Industrial Informatics (TII), Computer Vision and Image Understanding (CVIU), CVPR, AAAI, Australasian Database Conference (ADC), etc.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e5a1c89671652d01b988ea5619fbd452","permalink":"/author/zhihui-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhihui-li/","section":"authors","summary":"Greetings! I received my PhD degree from School of Computer Science and Engineering, University of New South Wales (UNSW), Australia. My supervisors are A/Professor Lina Yao and Professor Salil Kanhere. Before joining UNSW, I have worked as a research assistant with School of Computer Science and Technology at Shandong University.","tags":null,"title":"Zhihui Li","type":"authors"},{"authors":["Lei Chen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"465ba211583d8702c9a263cf80730880","permalink":"/author/lei-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lei-chen/","section":"authors","summary":"","tags":null,"title":"Lei Chen","type":"authors"},{"authors":["Changlin Li"],"categories":null,"content":"李长林博士目前是悉尼科技大学（UTS）的博士后研究员。他于2023年在UTS获得博士学位。在攻读博士学位之前，他在中国科学技术大学（USTC）获得了计算机科学与技术的学士学位。他的研究主要集中在生成式人工智能、高效深度学习方法以及神经网络结构的优化。学术主页：https://scholar.google.com/citations?user=RLAgwBkAAAAJ\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7a647389f120eed650c5d6adfce866e4","permalink":"/author/changlin-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/changlin-li/","section":"authors","summary":"李长林博士目前是悉尼科技大学（UTS）的博士后研究员。他于2023年在UTS获得博士学位。在攻读博士学位之前，他在中国科学技术大学（USTC）获得了计算机科学与技术的学士学位。他的研究主要集中在生成式人工智能、高效深度学习方法以及神经网络结构的优化。学术主页：https://scholar.google.com/citations?user=RLAgwBkAAAAJ","tags":null,"title":"Changlin Li","type":"authors"},{"authors":["MingJie Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7d26a92780c5f5efd17abc40c3d4d6f8","permalink":"/author/mingjie-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mingjie-li/","section":"authors","summary":"","tags":null,"title":"MingJie Li","type":"authors"},{"authors":["Peipei Song"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"df87182619372675439e92eed3a75001","permalink":"/author/peipei-song/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/peipei-song/","section":"authors","summary":"","tags":null,"title":"Peipei Song","type":"authors"},{"authors":["SiYi Hu"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dde51d82ecad551c1a9bf507250b3fef","permalink":"/author/siyi-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/siyi-hu/","section":"authors","summary":"(空)","tags":null,"title":"SiYi Hu","type":"authors"},{"authors":["Aijia Yang"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"06c390f0474090419869f9192c580547","permalink":"/author/aijia-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aijia-yang/","section":"authors","summary":"(空)","tags":null,"title":"Aijia Yang","type":"authors"},{"authors":["HaiHong Hao"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f764fdbf8677e12a6fa04fa9397eb660","permalink":"/author/haihong-hao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haihong-hao/","section":"authors","summary":"(空)","tags":null,"title":"HaiHong Hao","type":"authors"},{"authors":["HaoTian Ling"],"categories":null,"content":"研究方向： consistency distillation；video generation\npublication: learning to stop cut generation for efficient mix-integer programming. aaai'24\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f6603ee90adf35bc55738efbed665737","permalink":"/author/haotian-ling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haotian-ling/","section":"authors","summary":"研究方向： consistency distillation；video generation\npublication: learning to stop cut generation for efficient mix-integer programming. aaai'24","tags":null,"title":"HaoTian Ling","type":"authors"},{"authors":["JunFei Yi"],"categories":null,"content":"https://scholar.google.com/citations?user=uPcYnKIAAAAJ\u0026hl=zh-CN\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"aec07c3b86d620fbf9d59239af9202d2","permalink":"/author/junfei-yi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/junfei-yi/","section":"authors","summary":"https://scholar.google.com/citations?user=uPcYnKIAAAAJ\u0026hl=zh-CN","tags":null,"title":"JunFei Yi","type":"authors"},{"authors":["KeCheng Zhang"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"166b84c07ec81171c11e47a958733b94","permalink":"/author/kecheng-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kecheng-zhang/","section":"authors","summary":"(空)","tags":null,"title":"KeCheng Zhang","type":"authors"},{"authors":["Ming Pei"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"09440447035d2e788ebe5eaecf0506c4","permalink":"/author/ming-pei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ming-pei/","section":"authors","summary":"(空)","tags":null,"title":"Ming Pei","type":"authors"},{"authors":["MingZhe Yang"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"119bcace83b8384f8a4dd62d9def0a3b","permalink":"/author/mingzhe-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mingzhe-yang/","section":"authors","summary":"(空)","tags":null,"title":"MingZhe Yang","type":"authors"},{"authors":["Rong Dai"],"categories":null,"content":"https://scholar.google.com/citations?user=_epQEbgAAAAJ\u0026hl=zh-CN\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b921444951c0f5b16af5d73fd5bb8e30","permalink":"/author/rong-dai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rong-dai/","section":"authors","summary":"https://scholar.google.com/citations?user=_epQEbgAAAAJ\u0026hl=zh-CN","tags":null,"title":"Rong Dai","type":"authors"},{"authors":["Rui Liu"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"35cb17834e0c8b2057f07b0404fe59f3","permalink":"/author/rui-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rui-liu/","section":"authors","summary":"(空)","tags":null,"title":"Rui Liu","type":"authors"},{"authors":["SiHao Lin"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ea74435036889a5ff27e9da430d53209","permalink":"/author/sihao-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sihao-lin/","section":"authors","summary":"(空)","tags":null,"title":"SiHao Lin","type":"authors"},{"authors":["TengFei Liu"],"categories":null,"content":"https://tengfeiliu966.github.io/\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4638ba482d8590d1183ecfa5dda9ce87","permalink":"/author/tengfei-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tengfei-liu/","section":"authors","summary":"https://tengfeiliu966.github.io/","tags":null,"title":"TengFei Liu","type":"authors"},{"authors":["XinYi Wang"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"25c61616a528217520450c7d55eafd59","permalink":"/author/xinyi-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xinyi-wang/","section":"authors","summary":"(空)","tags":null,"title":"XinYi Wang","type":"authors"},{"authors":["YangYang Xu"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f17d61127b98a17cbaa04004847907f6","permalink":"/author/yangyang-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yangyang-xu/","section":"authors","summary":"(空)","tags":null,"title":"YangYang Xu","type":"authors"},{"authors":["YanLong Xu"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4d8cc138ca966084145f0ddbeb54bd3f","permalink":"/author/yanlong-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yanlong-xu/","section":"authors","summary":"(空)","tags":null,"title":"YanLong Xu","type":"authors"},{"authors":["YiJian Fan"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68d6b1920a2701ec9079d46cced08fc5","permalink":"/author/yijian-fan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yijian-fan/","section":"authors","summary":"(空)","tags":null,"title":"YiJian Fan","type":"authors"},{"authors":["YueDian Weng"],"categories":null,"content":"Computer Vision, Efficient Video Understanding, Multimodal Large Language Model\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"027ff1833b5e43ec204f7ee198503cfb","permalink":"/author/yuedian-weng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuedian-weng/","section":"authors","summary":"Computer Vision, Efficient Video Understanding, Multimodal Large Language Model","tags":null,"title":"YueDian Weng","type":"authors"},{"authors":["Boyu Tang"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3850139b841e914c913c291fa4c3e552","permalink":"/author/boyu-tang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/boyu-tang/","section":"authors","summary":"(空)","tags":null,"title":"Boyu Tang","type":"authors"},{"authors":["Chaofan Luo"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"63ccfd570db2aa299ef7d6749d1dfa0e","permalink":"/author/chaofan-luo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chaofan-luo/","section":"authors","summary":"(空)","tags":null,"title":"Chaofan Luo","type":"authors"},{"authors":["ChengLong Xu"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e772606777fc8026b5f7f9f69c4d62be","permalink":"/author/chenglong-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chenglong-xu/","section":"authors","summary":"(空)","tags":null,"title":"ChengLong Xu","type":"authors"},{"authors":["HaoWen Pan"],"categories":null,"content":"https://github.com/opanhw\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fb0ea1804b9e671bafc8633ef0847acb","permalink":"/author/haowen-pan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haowen-pan/","section":"authors","summary":"https://github.com/opanhw","tags":null,"title":"HaoWen Pan","type":"authors"},{"authors":["JiaLun Li"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"875450a3ca0f1a1714708dbd29332efd","permalink":"/author/jialun-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jialun-li/","section":"authors","summary":"(空)","tags":null,"title":"JiaLun Li","type":"authors"},{"authors":["JunJie Ye"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0047931fbfd1c38f2f98635a895f05ce","permalink":"/author/junjie-ye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/junjie-ye/","section":"authors","summary":"(空)","tags":null,"title":"JunJie Ye","type":"authors"},{"authors":["Long Zhang"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"23a6c08b8fc44c8ddb7998f73260cd36","permalink":"/author/long-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/long-zhang/","section":"authors","summary":"(空)","tags":null,"title":"Long Zhang","type":"authors"},{"authors":["MingYu Yang"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"72e7a941c75dc717eb1361189f2d56c4","permalink":"/author/mingyu-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mingyu-yang/","section":"authors","summary":"(空)","tags":null,"title":"MingYu Yang","type":"authors"},{"authors":["NaiXin Zhai"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"81205d8208f6cb35675ed2257ceb996c","permalink":"/author/naixin-zhai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/naixin-zhai/","section":"authors","summary":"(空)","tags":null,"title":"NaiXin Zhai","type":"authors"},{"authors":["Ran Ju"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f5026c21367b37dc2c3f8e3bbfdb8f07","permalink":"/author/ran-ju/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ran-ju/","section":"authors","summary":"(空)","tags":null,"title":"Ran Ju","type":"authors"},{"authors":["ShiCheng Wang"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"50e1f833c8722909bb12a7901b2bc764","permalink":"/author/shicheng-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shicheng-wang/","section":"authors","summary":"(空)","tags":null,"title":"ShiCheng Wang","type":"authors"},{"authors":["WenLin Wu"],"categories":null,"content":"github个人主页地址：https://github.com/infinity086\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c12ccdf17594132dd6b50effd555fdd0","permalink":"/author/wenlin-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wenlin-wu/","section":"authors","summary":"github个人主页地址：https://github.com/infinity086","tags":null,"title":"WenLin Wu","type":"authors"},{"authors":["XiaoLei Hao"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"65239eca1c386325aef251556331916d","permalink":"/author/xiaolei-hao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaolei-hao/","section":"authors","summary":"(空)","tags":null,"title":"XiaoLei Hao","type":"authors"},{"authors":["ZhiYuan Han"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0f43144c533195261b072bf974e49838","permalink":"/author/zhiyuan-han/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiyuan-han/","section":"authors","summary":"(空)","tags":null,"title":"ZhiYuan Han","type":"authors"},{"authors":["GenMing Cui"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5261c2203496811311638eaf388a201a","permalink":"/author/genming-cui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/genming-cui/","section":"authors","summary":"(空)","tags":null,"title":"GenMing Cui","type":"authors"},{"authors":["JiaYu Ceng"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"be7ba4ecd0e0e659da70a9257e55ef65","permalink":"/author/jiayu-ceng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiayu-ceng/","section":"authors","summary":"(空)","tags":null,"title":"JiaYu Ceng","type":"authors"},{"authors":["JinMing Gong"],"categories":null,"content":"https://github.com/Gjmustc\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c8638b345b0d1127bc30d9b1a46771cc","permalink":"/author/jinming-gong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jinming-gong/","section":"authors","summary":"https://github.com/Gjmustc","tags":null,"title":"JinMing Gong","type":"authors"},{"authors":["Qi Xie"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89932e3579a9595e20de21f4e23792ed","permalink":"/author/qi-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qi-xie/","section":"authors","summary":"(空)","tags":null,"title":"Qi Xie","type":"authors"},{"authors":["SiYu Ning"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ec0dc6b3916ffa3f512187ac342e2a58","permalink":"/author/siyu-ning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/siyu-ning/","section":"authors","summary":"(空)","tags":null,"title":"SiYu Ning","type":"authors"},{"authors":["XiaoYu Shi"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c9de6f297b5a8bca4fa32771abe1d40f","permalink":"/author/xiaoyu-shi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaoyu-shi/","section":"authors","summary":"(空)","tags":null,"title":"XiaoYu Shi","type":"authors"},{"authors":["YuLu Zhou"],"categories":null,"content":"(空)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bc58ba3b840cac70ff0a559cd6990223","permalink":"/author/yulu-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yulu-zhou/","section":"authors","summary":"(空)","tags":null,"title":"YuLu Zhou","type":"authors"},{"authors":["ZhiJing Cheng"],"categories":null,"content":"空\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ede6a5585210578a69ad2583ba354c0d","permalink":"/author/zhijing-cheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhijing-cheng/","section":"authors","summary":"空","tags":null,"title":"ZhiJing Cheng","type":"authors"},{"authors":["MingFei Han"],"categories":null,"content":"He is working on embodiment learning and vision-text multimodal learning. Especially, he is interested in boosting robots with human videos from web.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"27d046c5b5bcb19ec763470f98362f7f","permalink":"/author/mingfei-han/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mingfei-han/","section":"authors","summary":"He is working on embodiment learning and vision-text multimodal learning. Especially, he is interested in boosting robots with human videos from web.","tags":null,"title":"MingFei Han","type":"authors"},{"authors":["admin"],"categories":null,"content":"The USTC Future Media Computing Lab is run by Prof Xiaojun Chang and Prof Xun Yang. It is a cutting-edge research center dedicated to advancing the frontiers of multimedia and AI technologies. Focused on areas such as video content analysis, multimodal intelligence, 3D vision, and human-computer interaction, the lab aims to revolutionize how media is processed, understood, and generated. Through interdisciplinary collaboration, the lab develops innovative algorithms and systems that address real-world challenges, from video-based recognition tasks to intelligent media creation, fostering breakthroughs in both academic and industrial applications.\nThe USTC Future Media Computing Lab is always looking for talented undergraduate, graduate students and postdocs. If you\u0026rsquo;re interested in working in the exciting field of future media computing, feel free to reach out!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/ustc-future-media-computing-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ustc-future-media-computing-lab/","section":"authors","summary":"The USTC Future Media Computing Lab is run by Prof Xiaojun Chang and Prof Xun Yang. It is a cutting-edge research center dedicated to advancing the frontiers of multimedia and AI technologies.","tags":null,"title":"USTC Future Media Computing Lab","type":"authors"},{"authors":["Guangrun Wang","Changlin Li","Liuchun Yuan","Jiefeng Peng","Xiaoyu Xian","Xiaodan Liang","Xiaojun Chang","Liang Lin"],"categories":[],"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"516d09b0f668580e22794dcecf8b5241","permalink":"/publication/dna_family_boosting_weight-sharing_nas_with_block-wise_supervisions/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/publication/dna_family_boosting_weight-sharing_nas_with_block-wise_supervisions/","section":"publication","summary":"This paper presents the DNA Family, a new framework for boosting the effectiveness of weight-sharing Neural Architecture Search (NAS) by dividing large search spaces into smaller blocks and applying block-wise supervisions. The approach demonstrates high performance on benchmarks such as ImageNet, surpassing previous NAS techniques in accuracy and efficiency.","tags":["Neural Architecture Search","Weight-Sharing NAS","Block-wise Learning"],"title":"DNA Family: Boosting Weight-Sharing NAS With Block-Wise Supervisions","type":"publication"},{"authors":["Changlin Li","Guangrun Wang","Bing Wang","Xiaodan Liang","Zhihui Li","Xiaojun Chang"],"categories":[],"content":"","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"9276ff7e8c10cc0183cdd34813dfa2a5","permalink":"/publication/ds-net_dynamic_weight_slicing_for_efficient_inference_in_cnns_and_vision_transformers/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/publication/ds-net_dynamic_weight_slicing_for_efficient_inference_in_cnns_and_vision_transformers/","section":"publication","summary":"This paper presents DS-Net++, a novel framework for efficient inference in neural networks. Dynamic weight slicing allows for scalable performance across multiple architectures like CNNs and vision transformers. The method delivers up to 61.5% real-world acceleration with minimal accuracy drops on models like MobileNet, ResNet-50, and Vision Transformer, showing its potential in hardware-efficient dynamic networks.","tags":["Dynamic Inference","Neural Networks","Efficient Inference","CNN","Vision Transformers"],"title":"DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Vision Transformers","type":"publication"},{"authors":["Caixia Yan","Xiaojun Chang","Minnan Luo","Huan Liu","Xiaoqin Zhang","Qinghua Zheng"],"categories":[],"content":"","date":1709251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709251200,"objectID":"89351fff51031082e33b82413c845a89","permalink":"/publication/semantics-guided_contrastive_network_for_zero-shot_object_detection/","publishdate":"2024-03-01T00:00:00Z","relpermalink":"/publication/semantics-guided_contrastive_network_for_zero-shot_object_detection/","section":"publication","summary":"This paper presents ContrastZSD, a semantics-guided contrastive network for zero-shot object detection (ZSD). The framework improves visual-semantic alignment and mitigates the bias problem towards seen classes by incorporating region-category and region-region contrastive learning. ContrastZSD demonstrates superior performance in both ZSD and generalized ZSD tasks across PASCAL VOC and MS COCO datasets.","tags":["Zero-Shot Object Detection","Contrastive Learning","Supervised Contrastive Learning"],"title":"Semantics-Guided Contrastive Network for Zero-Shot Object Detection","type":"publication"},{"authors":["Lingling Zhang","Xiaojun Chang","Jun Liu","Minnan Luo","Zhihui Li","Lina Yao","Alex Hauptmann"],"categories":[],"content":"","date":1709251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709251200,"objectID":"506b03293a952af13916a3d911ce3b17","permalink":"/publication/tn-zstad_transferable_network_for_zero-shot_temporal_activity_detection/","publishdate":"2024-03-01T00:00:00Z","relpermalink":"/publication/tn-zstad_transferable_network_for_zero-shot_temporal_activity_detection/","section":"publication","summary":"TN-ZSTAD introduces a novel approach to zero-shot temporal activity detection (ZSTAD) in long untrimmed videos. By integrating an activity graph transformer with zero-shot detection techniques, it addresses the challenge of recognizing and localizing unseen activities. Experiments on THUMOS'14, Charades, and ActivityNet datasets validate its superior performance in detecting unseen activities.","tags":["Zero-Shot Temporal Activity Detection","Activity Graph Transformer","Zero-Shot Learning"],"title":"TN-ZSTAD: Transferable Network for Zero-Shot Temporal Activity Detection","type":"publication"},{"authors":["Zhe Liu","Yun Li","Lina Yao","Xiaojun Chang","Wei Fang","Xiaojun Wu","Abdulmotaleb El Saddik"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"da4ff90a0e798578e5c8d1e3ad9775bd","permalink":"/publication/simple_primitives_with_feasibility-_and_contextuality-dependence_for_open-world_compositional_zero-shot_learning/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/simple_primitives_with_feasibility-_and_contextuality-dependence_for_open-world_compositional_zero-shot_learning/","section":"publication","summary":"This paper proposes the SAD-SP model, which improves open-world compositional zero-shot learning by capturing contextuality and feasibility dependencies between states and objects. Using semantic attention and knowledge disentanglement, the approach enhances performance on benchmarks like MIT-States and C-GQA by predicting unseen compositions more accurately.","tags":["Compositional Zero-Shot Learning","Contextuality-Dependence","Knowledge Disentanglement","Semantic Attention"],"title":"Simple Primitives With Feasibility- and Contextuality-Dependence for Open-World Compositional Zero-Shot Learning","type":"publication"},{"authors":["Haoyu Zhang","Meng Liu","Yuhong Li","Ming Yan","Zan Gao","Xiaojun Chang","Liqiang Nie"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"4df2273f5f8ab5d7589e4813f78ef48d","permalink":"/publication/attribute-guided_collaborative_learning_for_partial_person_re-identification/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/publication/attribute-guided_collaborative_learning_for_partial_person_re-identification/","section":"publication","summary":"This paper introduces a novel framework for partial person re-identification, addressing the challenge of image spatial misalignment due to occlusions. The framework utilizes an adaptive threshold-guided masked graph convolutional network and incorporates human attributes to enhance the accuracy of pedestrian representations. Experimental results demonstrate its effectiveness across multiple public datasets.","tags":["Partial Person Re-Identification","Masked Graph Convolutional Network","Human Attributes"],"title":"Attribute-Guided Collaborative Learning for Partial Person Re-Identification","type":"publication"},{"authors":["Caixia Yan","Xiaojun Chang","Zhihui Li","Weili Guan","Zongyuan Ge","Lei Zhu","Qinghua Zheng"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"d2439038ef0fbbd5c610b7ff1bdf1dcc","permalink":"/publication/zeronas_differentiable_generative_adversarial_networks_search_for_zero-shot_learning/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/publication/zeronas_differentiable_generative_adversarial_networks_search_for_zero-shot_learning/","section":"publication","summary":"ZeroNAS presents a differentiable generative adversarial network architecture search method specifically designed for zero-shot learning (ZSL). The approach optimizes both generator and discriminator architectures, leading to significant improvements in ZSL and generalized ZSL tasks across various datasets.","tags":["Neural Architecture Search","Zero-Shot Learning","Generative Adversarial Networks"],"title":"ZeroNAS: Differentiable Generative Adversarial Networks Search for Zero-Shot Learning","type":"publication"},{"authors":["Zhihui Li","Pengfei Xu","Xiaojun Chang","Luyao Yang","Yuanyuan Zhang","Lina Yao","Xiaojiang Chen"],"categories":[],"content":"","date":1690848e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690848e3,"objectID":"f400e54583ab6b4e1be4fd26a0706b85","permalink":"/publication/when_object_detection_meets_knowledge_distillation_a_survey/","publishdate":"2023-08-01T00:00:00Z","relpermalink":"/publication/when_object_detection_meets_knowledge_distillation_a_survey/","section":"publication","summary":"This paper provides a comprehensive review of the recent advancements in knowledge distillation (KD)-based object detection (OD) models. It covers different KD strategies for improving object detection tasks, such as incremental OD, small object detection, and weakly supervised OD. The paper also explores advanced distillation techniques and highlights future research directions in the field.","tags":["Knowledge Distillation","Object Detection","Model Compression","Feature Distillation"],"title":"When Object Detection Meets Knowledge Distillation: A Survey","type":"publication"},{"authors":["Mingjie Li","Po-Yao Huang","Xiaojun Chang","Junjie Hu","Yi Yang","Alex Hauptmann"],"categories":[],"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"d35db15fb24704f183651622d78d0dbf","permalink":"/publication/video_pivoting_unsupervised_multi-modal_machine_translation/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/publication/video_pivoting_unsupervised_multi-modal_machine_translation/","section":"publication","summary":"This paper introduces a video pivoting method for unsupervised multi-modal machine translation (UMMT), which uses spatial-temporal graphs to align sentence pairs in the latent space. By leveraging visual content from videos, the approach enhances translation accuracy and generalization across multiple languages, as demonstrated on the VATEX and HowToWorld datasets.","tags":["Multi-Modal Machine Translation","Unsupervised Learning","Spatial-Temporal Graph"],"title":"Video Pivoting Unsupervised Multi-Modal Machine Translation","type":"publication"},{"authors":["Xiaojun Chang","Pengzhen Ren","Pengfei Xu","Zhihui Li","Xiaojiang Chen","Alex Hauptmann"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"7cd9bc3b8de4921df56de73cccb6a322","permalink":"/publication/a_comprehensive_survey_of_scene_graphs_generation_and_application/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/a_comprehensive_survey_of_scene_graphs_generation_and_application/","section":"publication","summary":"This survey provides a thorough exploration of the concept of scene graphs, discussing their role in visual understanding tasks. Scene graphs represent objects, their attributes, and relationships, helping improve tasks like visual reasoning and image captioning. The paper outlines various generation methods and applications, and also highlights key challenges like the long-tailed distribution of relationships.","tags":["Scene Graphs","Visual Relationship Recognition","Deep Learning"],"title":"A Comprehensive Survey of Scene Graphs: Generation and Application","type":"publication"},{"authors":["Pengzhen Ren","Yun Xiao","Xiaojun Chang","Po-Yao Huang","Zhihui Li","Brij B. Gupta","Xiaojiang Chen","Xin Wang"],"categories":[],"content":"","date":1633651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633651200,"objectID":"1d42ea85f340207ef885639099e2c909","permalink":"/publication/deep_active_learning_survey/","publishdate":"2021-10-08T00:00:00Z","relpermalink":"/publication/deep_active_learning_survey/","section":"publication","summary":"Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.\nIt is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.","tags":["Deep learning","Active learning","Deep active learning"],"title":"A Survey of Deep Active Learning","type":"publication"},{"authors":["Miao Zhang","Huiqi Li","Shirui Pan","Xiaojun Chang","Chuan Zhou","Zongyuan Ge","Steven Su"],"categories":[],"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"6d53e90da92d9f56bfc4a9d2075e43fc","permalink":"/publication/one-shot_neural_architecture_search_maximising_diversity_to_overcome_catastrophic_forgetting/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/one-shot_neural_architecture_search_maximising_diversity_to_overcome_catastrophic_forgetting/","section":"publication","summary":"This work addresses the catastrophic forgetting problem in one-shot neural architecture search by treating supernet training as a constrained optimization problem. The proposed method uses a novelty search-based architecture selection approach to enhance diversity and boost performance, achieving competitive results on CIFAR-10 and ImageNet datasets.","tags":["Neural Architecture Search","Catastrophic Forgetting","Novelty Search","Continual Learning"],"title":"One-Shot Neural Architecture Search: Maximising Diversity to Overcome Catastrophic Forgetting","type":"publication"},{"authors":["Pengzhen Ren","Yun Xiao","Xiaojun Chang","Po-yao Huang","Zhihui Li","Xiaojiang Chen","Xin Wang"],"categories":[],"content":"","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"caac03bc32e43fbe4643f4a921835d5d","permalink":"/publication/neural_architecture_search_survey/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/publication/neural_architecture_search_survey/","section":"publication","summary":"Deep learning has made substantial breakthroughs in many fields due to its powerful automatic representation capabilities. It has been proven that neural architecture design is crucial to the feature representation of data and the final performance. However, the design of the neural architecture heavily relies on the researchers’ prior knowledge and experience. And due to the limitations of humans’ inherent knowledge, it is difficult for people to jump out of their original thinking paradigm and design an optimal model. Therefore, an intuitive idea would be to reduce human intervention as much as possible and let the algorithm automatically design the neural architecture. Neural Architecture Search (NAS) is just such a revolutionary algorithm, and the related research work is complicated and rich. Therefore, a comprehensive and systematic survey on the NAS is essential. Previously related surveys have begun to classify existing work mainly based on the key components of NAS: search space, search strategy, and evaluation strategy. While this classification method is more intuitive, it is difficult for readers to grasp the challenges and the landmark work involved. Therefore, in this survey, we provide a new perspective: beginning with an overview of the characteristics of the earliest NAS algorithms, summarizing the problems in these early NAS algorithms, and then providing solutions for subsequent related research work. In addition, we conduct a detailed and comprehensive analysis, comparison, and summary of these works. Finally, we provide some possible future research directions.","tags":["Neural architecture search","AutoDL","Continuous search strategy"],"title":"A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions","type":"publication"},{"authors":["Jianfeng Dong","Xirong Li","Chaoxi Xu","Xun Yang","Gang Yang","Xun Wang"],"categories":[],"content":"","date":1613347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613347200,"objectID":"8c009ade6fb7487190366a1523de1b42","permalink":"/publication/dual_encoding_video_retrieval_by_text/","publishdate":"2021-02-15T00:00:00Z","relpermalink":"/publication/dual_encoding_video_retrieval_by_text/","section":"publication","summary":"This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described exclusively in the form of a natural-language sentence, with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is crucial. To that end, the two modalities need to be first encoded into real-valued vectors and then projected into a common space. In this paper we achieve this by proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Our novelty is two-fold. First, different from prior art that resorts to a specific single-level encoder, the proposed network performs multi-level encoding that represents the rich content of both modalities in a coarse-to-fine fashion. Second, different from a conventional common space learning algorithm which is either concept based or latent space based, we introduce hybrid space learning which combines the high performance of the latent space and the good interpretability of the concept space. Dual encoding is conceptually simple, practically effective and end-to-end trained with hybrid space learning. Extensive experiments on four challenging video datasets show the viability of the new method. Code and data are available at https://github.com/danieljf24/hybrid_space","tags":["Video retrieval","Dual encoding","Text query"],"title":"Dual Encoding for Video Retrieval by Text","type":"publication"},{"authors":["Zonghan Wu","Shirui Pan","Guodong Long","Jing Jiang","Xiaojun Chang","Chengqi Zhang"],"categories":[],"content":"","date":1598140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610746465,"objectID":"8b21dc4aee0c198c0df8a967a785d718","permalink":"/publication/connecting_the_dots/","publishdate":"2020-08-23T00:00:00Z","relpermalink":"/publication/connecting_the_dots/","section":"publication","summary":"Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.","tags":["Graph Neural Networks","Time Series Forecasting"],"title":"Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks","type":"publication"},{"authors":["Xiaojun Chang","Yao-Liang Yu","Yi Yang","Eric P. Xing"],"categories":[],"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"8368a68ea02af549137de5b1c78facd5","permalink":"/publication/semantic_pooling_for_complex_event_analysis_in_untrimmed_videos/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/semantic_pooling_for_complex_event_analysis_in_untrimmed_videos/","section":"publication","summary":"This paper introduces a novel semantic pooling method for event analysis tasks like detection, recognition, and recounting in long untrimmed Internet videos. Using semantic saliency, the approach ranks video shots to prioritize the most relevant ones, improving the classifier’s accuracy. The paper proposes a nearly-isotonic SVM classifier, validated with experiments on real-world datasets, showcasing significant performance improvements.","tags":["Event Detection","Event Recognition","Semantic Saliency"],"title":"Semantic Pooling for Complex Event Analysis in Untrimmed Videos","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"300db838c7a68c4fd14037dd038d2e1c","permalink":"/courses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"60abaeb405147d861bb55801d25d30df","permalink":"/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9eb50f9088083bebcb7e4cf99e22b9ed","permalink":"/news/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"","tags":null,"title":"News","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1792b5ce7af54aa10fd0608cca0972d","permalink":"/people-future-media-computing-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people-future-media-computing-lab/","section":"","summary":"","tags":null,"title":"People","type":"widget_page"}]