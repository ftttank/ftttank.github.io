<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 4.8.0 for Hugo"><meta name=author content="Future Media Computing Lab"><meta name=description content><link rel=alternate hreflang=en-us href=/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/wowchemy.css><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Future Media Computing Lab"><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hu98f8dd200ffb525419093529eb1e5d7e_937188_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_hu98f8dd200ffb525419093529eb1e5d7e_937188_192x192_fill_lanczos_center_3.png><link rel=canonical href=/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Future Media Computing Lab"><meta property="og:url" content="/"><meta property="og:title" content="Future Media Computing Lab"><meta property="og:description" content><meta property="og:image" content="/images/logo.svg"><meta property="twitter:image" content="/images/logo.svg"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2021-10-08T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"/","name":"Future Media Computing Lab","logo":"/images/logo.svg","address":{"@type":"PostalAddress","streetAddress":"中国科学技术大学高新校区","addressLocality":"合肥市","addressRegion":"安徽省","postalCode":"230088","addressCountry":"CN"},"url":"/"}</script><title>Future Media Computing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main><script>window.wcDarkLightEnabled=!0</script><script>const isSiteThemeDark=!1</script><script src=/js/load-theme.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/images/logo.svg alt="Future Media Computing Lab"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/images/logo.svg alt="Future Media Computing Lab"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/people-future-media-computing-lab><span>People</span></a></li><li class=nav-item><a class=nav-link href=/news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact data-target=#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" src=/author/future-media-computing-lab/avatar_hu5f8103cf5d5cddfaac8b580d13ead092_462490_270x270_fill_lanczos_center_3.png alt="Future Media Computing Lab"><div class=portrait-title><h2>Future Media Computing Lab</h2><h3><span>University of Science and Technology of China</span></h3></div><ul class=network-icon aria-hidden=true><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/xiaojun-chang target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Welcome to the Future Media Computing Lab</h1><p>The Future Media Computing Lab is run by <a href=/author/xiaojun-chang/>Xiaojun Chang</a>. The research in the lab focuses on &mldr;. Examples of work include: &mldr;.</p><div class=row></div></div></div></div></section><section id=people class="home-section wg-people"><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Meet the Team</h1></div><div class=col-md-12><h2 class=mb-4>Researchers</h2></div><div class="col-12 col-sm-auto people-person"><a href=/author/xiaojun-chang/><img class="avatar avatar-circle" src=/author/xiaojun-chang/avatar_hudef30b94a392be4f95107e96ebbfd10d_3770_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/xiaojun-chang/>Xiaojun Chang</a></h2><h3>长江讲席 / Director</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/xun-yang/><img class="avatar avatar-circle" src=/author/xun-yang/avatar_hu5e4fb6c689510e0963f9ffb2bdb6b4e7_2790_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/xun-yang/>Xun Yang</a></h2><h3>海外优青 / Co-Director</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/zhihui-li/><img class="avatar avatar-circle" src=/author/zhihui-li/avatar_hucb610fb86b60c80d4910cfbef7b8b79d_3204_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/zhihui-li/>Zhihui Li</a></h2><h3>海外优青</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/lei-chen/><img class="avatar avatar-circle" src=/author/lei-chen/avatar_hud641fb584c6d776ec297464866d0d8a8_39838_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/lei-chen/>Lei Chen</a></h2><h3>特任副研究员</h3></div></div></div></div></section><section id=featured class="home-section wg-featured"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Featured Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span><a href=/author/pengzhen-ren/>Pengzhen Ren</a></span>, <span><a href=/author/yun-xiao/>Yun Xiao</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/po-yao-huang/>Po-Yao Huang</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/brij-b.-gupta/>Brij B. Gupta</a></span>, <span><a href=/author/xiaojiang-chen/>Xiaojiang Chen</a></span>, <span><a href=/author/xin-wang/>Xin Wang</a></span></div><span class=article-date>October 2021</span>
<span class=middot-divider></span>
<span class=pub-publication>ACM Computing Surveys</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/deep_active_learning_survey/>A Survey of Deep Active Learning</a></h3><a href=/publication/deep_active_learning_survey/ class=summary-link><div class=article-style><p>Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due.
It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://dl.acm.org/doi/abs/10.1145/3472291 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/deep_active_learning_survey/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1145/3472291 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/pengzhen-ren/>Pengzhen Ren</a></span>, <span><a href=/author/yun-xiao/>Yun Xiao</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/po-yao-huang/>Po-Yao Huang</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/xiaojiang-chen/>Xiaojiang Chen</a></span>, <span><a href=/author/xin-wang/>Xin Wang</a></span></div><span class=article-date>May 2021</span>
<span class=middot-divider></span>
<span class=pub-publication>ACM Computing Surveys</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/neural_architecture_search_survey/>A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions</a></h3><a href=/publication/neural_architecture_search_survey/ class=summary-link><div class=article-style><p>Deep learning has made substantial breakthroughs in many fields due to its powerful automatic representation capabilities. It has been proven that neural architecture design is crucial to the feature representation of data and the final performance. However, the design of the neural architecture heavily relies on the researchers’ prior knowledge and experience. And due to the limitations of humans’ inherent knowledge, it is difficult for people to jump out of their original thinking paradigm and design an optimal model. Therefore, an intuitive idea would be to reduce human intervention as much as possible and let the algorithm automatically design the neural architecture. Neural Architecture Search (NAS) is just such a revolutionary algorithm, and the related research work is complicated and rich. Therefore, a comprehensive and systematic survey on the NAS is essential. Previously related surveys have begun to classify existing work mainly based on the key components of NAS: search space, search strategy, and evaluation strategy. While this classification method is more intuitive, it is difficult for readers to grasp the challenges and the landmark work involved. Therefore, in this survey, we provide a new perspective: beginning with an overview of the characteristics of the earliest NAS algorithms, summarizing the problems in these early NAS algorithms, and then providing solutions for subsequent related research work. In addition, we conduct a detailed and comprehensive analysis, comparison, and summary of these works. Finally, we provide some possible future research directions.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://dl.acm.org/doi/abs/10.1145/3447582 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/neural_architecture_search_survey/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1145/3447582 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/jianfeng-dong/>Jianfeng Dong</a></span>, <span><a href=/author/xirong-li/>Xirong Li</a></span>, <span><a href=/author/chaoxi-xu/>Chaoxi Xu</a></span>, <span><a href=/author/xun-yang/>Xun Yang</a></span>, <span><a href=/author/gang-yang/>Gang Yang</a></span>, <span><a href=/author/xun-wang/>Xun Wang</a></span></div><span class=article-date>February 2021</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/dual_encoding_video_retrieval_by_text/>Dual Encoding for Video Retrieval by Text</a></h3><a href=/publication/dual_encoding_video_retrieval_by_text/ class=summary-link><div class=article-style><p>This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described exclusively in the form of a natural-language sentence, with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is crucial. To that end, the two modalities need to be first encoded into real-valued vectors and then projected into a common space. In this paper we achieve this by proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Our novelty is two-fold. First, different from prior art that resorts to a specific single-level encoder, the proposed network performs multi-level encoding that represents the rich content of both modalities in a coarse-to-fine fashion. Second, different from a conventional common space learning algorithm which is either concept based or latent space based, we introduce hybrid space learning which combines the high performance of the latent space and the good interpretability of the concept space. Dual encoding is conceptually simple, practically effective and end-to-end trained with hybrid space learning. Extensive experiments on four challenging video datasets show the viability of the new method. Code and data are available at <a href=https://github.com/danieljf24/hybrid_space target=_blank rel=noopener>https://github.com/danieljf24/hybrid_space</a></p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/abstract/document/9354593/ target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/dual_encoding_video_retrieval_by_text/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2021.3059295 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/zonghan-wu/>Zonghan Wu</a></span>, <span><a href=/author/shirui-pan/>Shirui Pan</a></span>, <span><a href=/author/guodong-long/>Guodong Long</a></span>, <span><a href=/author/jing-jiang/>Jing Jiang</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/chengqi-zhang/>Chengqi Zhang</a></span></div><span class=article-date>August 2020</span>
<span class=middot-divider></span>
<span class=pub-publication>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 2020)</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/connecting_the_dots/>Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks</a></h3><a href=/publication/connecting_the_dots/ class=summary-link><div class=article-style><p>Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://dl.acm.org/doi/abs/10.1145/3394486.3403118 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/connecting_the_dots/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1145/3394486.3403118 target=_blank rel=noopener>DOI</a></div></div></div></div></div></section><section id=publications class="home-section wg-pages"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class="alert alert-note"><div>Quickly discover relevant content by <a href=/publication/>filtering publications</a>.</div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/deep_active_learning_survey/>A Survey of Deep Active Learning</a></h3><a href=/publication/deep_active_learning_survey/ class=summary-link><div class=article-style>Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/pengzhen-ren/>Pengzhen Ren</a></span>, <span><a href=/author/yun-xiao/>Yun Xiao</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/po-yao-huang/>Po-Yao Huang</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/brij-b.-gupta/>Brij B. Gupta</a></span>, <span><a href=/author/xiaojiang-chen/>Xiaojiang Chen</a></span>, <span><a href=/author/xin-wang/>Xin Wang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://dl.acm.org/doi/abs/10.1145/3472291 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/deep_active_learning_survey/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1145/3472291 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/neural_architecture_search_survey/>A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions</a></h3><a href=/publication/neural_architecture_search_survey/ class=summary-link><div class=article-style>Deep learning has made substantial breakthroughs in many fields due to its powerful automatic representation capabilities. It has been …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/pengzhen-ren/>Pengzhen Ren</a></span>, <span><a href=/author/yun-xiao/>Yun Xiao</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/po-yao-huang/>Po-Yao Huang</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/xiaojiang-chen/>Xiaojiang Chen</a></span>, <span><a href=/author/xin-wang/>Xin Wang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://dl.acm.org/doi/abs/10.1145/3447582 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/neural_architecture_search_survey/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1145/3447582 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/dual_encoding_video_retrieval_by_text/>Dual Encoding for Video Retrieval by Text</a></h3><a href=/publication/dual_encoding_video_retrieval_by_text/ class=summary-link><div class=article-style>This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/jianfeng-dong/>Jianfeng Dong</a></span>, <span><a href=/author/xirong-li/>Xirong Li</a></span>, <span><a href=/author/chaoxi-xu/>Chaoxi Xu</a></span>, <span><a href=/author/xun-yang/>Xun Yang</a></span>, <span><a href=/author/gang-yang/>Gang Yang</a></span>, <span><a href=/author/xun-wang/>Xun Wang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/abstract/document/9354593/ target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/dual_encoding_video_retrieval_by_text/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2021.3059295 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/connecting_the_dots/>Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks</a></h3><a href=/publication/connecting_the_dots/ class=summary-link><div class=article-style>Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/zonghan-wu/>Zonghan Wu</a></span>, <span><a href=/author/shirui-pan/>Shirui Pan</a></span>, <span><a href=/author/guodong-long/>Guodong Long</a></span>, <span><a href=/author/jing-jiang/>Jing Jiang</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/chengqi-zhang/>Chengqi Zhang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://dl.acm.org/doi/abs/10.1145/3394486.3403118 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/connecting_the_dots/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1145/3394486.3403118 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div></div></div></div></section><section id=tags class="home-section wg-tag-cloud"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Popular Topics</h1></div><div class="col-12 col-lg-8"><div class=tag-cloud><a href=/tag/active-learning/ style=font-size:.7rem>Active learning</a>
<a href=/tag/autodl/ style=font-size:.7rem>AutoDL</a>
<a href=/tag/continuous-search-strategy/ style=font-size:.7rem>Continuous search strategy</a>
<a href=/tag/deep-active-learning/ style=font-size:.7rem>Deep active learning</a>
<a href=/tag/deep-learning/ style=font-size:.7rem>Deep learning</a>
<a href=/tag/dual-encoding/ style=font-size:.7rem>Dual encoding</a>
<a href=/tag/graph-neural-networks/ style=font-size:.7rem>Graph Neural Networks</a>
<a href=/tag/neural-architecture-search/ style=font-size:.7rem>Neural architecture search</a>
<a href=/tag/text-query/ style=font-size:.7rem>Text query</a>
<a href=/tag/time-series-forecasting/ style=font-size:.7rem>Time Series Forecasting</a>
<a href=/tag/video-retrieval/ style=font-size:.7rem>Video retrieval</a></div></div></div></div></section><section id=contact class="home-section wg-contact"><div class=container><div class="row contact-widget"><div class="col-12 col-lg-4 section-heading"><h1>Contact</h1></div><div class="col-12 col-lg-8"><div class=mb-3><form name=contact method=post action=https://formspree.io/geneticlogiclab@gmail.com><div class="form-group form-inline"><label class=sr-only for=inputName>Name</label>
<input type=text name=name class="form-control w-100" id=inputName placeholder=Name required></div><div class="form-group form-inline"><label class=sr-only for=inputEmail>Email</label>
<input type=email name=email class="form-control w-100" id=inputEmail placeholder=Email required></div><div class=form-group><label class=sr-only for=inputMessage>Message</label>
<textarea name=message class=form-control id=inputMessage rows=5 placeholder=Message required></textarea></div><button type=submit class="btn btn-outline-primary px-3 py-2">Send</button></form></div><ul class=fa-ul><li><i class="fa-li fas fa-map-marker fa-2x" aria-hidden=true></i>
<span id=person-address>中国科学技术大学高新校区, 合肥市, 安徽省 230088</span></li><li><i class="fa-li fab fa-github fa-2x" aria-hidden=true></i>
<a href=https://github.com/MyersResearchGroup target=_blank rel=noopener>Github Contact</a></li></ul><div class=d-none><input id=map-provider value=2>
<input id=map-lat value=31.820132553621974>
<input id=map-lng value=117.12195038760376>
<input id=map-dir value="中国科学技术大学高新校区, 合肥市, 安徽省 230088">
<input id=map-zoom value=15>
<input id=map-api-key value=pk.eyJ1IjoiZ2VuZXRpY2xvZ2ljbGFiIiwiYSI6ImNraHkxNzhwczAxZ2syeWw2eXk1ajFzd3QifQ.OkfUVvBL_nR39T8TtBZetg></div><div id=map></div></div></div></div></section><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script>
<script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",talk:"Talks",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script>window.netlifyIdentity&&window.netlifyIdentity.on("init",e=>{e||window.netlifyIdentity.on("login",()=>{document.location.href="/admin/"})})</script><script src=/js/wowchemy.min.ee9dd1b51c1cbfc9adeb51e02586011c.js></script><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href=https://wowchemy.com target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>