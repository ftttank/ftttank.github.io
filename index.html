<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 4.8.0 for Hugo"><meta name=author content="USTC Future Media Computing Lab"><meta name=description content><link rel=alternate hreflang=en-us href=/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/wowchemy.css><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Future Media Computing Lab"><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hu98f8dd200ffb525419093529eb1e5d7e_937188_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_hu98f8dd200ffb525419093529eb1e5d7e_937188_192x192_fill_lanczos_center_3.png><link rel=canonical href=/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Future Media Computing Lab"><meta property="og:url" content="/"><meta property="og:title" content="Future Media Computing Lab"><meta property="og:description" content><meta property="og:image" content="/images/logo.svg"><meta property="twitter:image" content="/images/logo.svg"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2024-05-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"/","name":"Future Media Computing Lab","logo":"/images/logo.svg","address":{"@type":"PostalAddress","streetAddress":"中国科学技术大学高新校区","addressLocality":"合肥市","addressRegion":"安徽省","postalCode":"230088","addressCountry":"CN"},"url":"/"}</script><title>Future Media Computing Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main><script>window.wcDarkLightEnabled=!0</script><script>const isSiteThemeDark=!1</script><script src=/js/load-theme.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/images/logo.svg alt="Future Media Computing Lab"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/images/logo.svg alt="Future Media Computing Lab"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/people-future-media-computing-lab><span>People</span></a></li><li class=nav-item><a class=nav-link href=/news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact data-target=#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" src=/author/ustc-future-media-computing-lab/avatar_hu5f8103cf5d5cddfaac8b580d13ead092_462490_270x270_fill_lanczos_center_3.png alt="USTC Future Media Computing Lab"><div class=portrait-title><h2>USTC Future Media Computing Lab</h2><h3><span>University of Science and Technology of China</span></h3></div><ul class=network-icon aria-hidden=true><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/xiaojun-chang target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Welcome to the Future Media Computing Lab</h1><p>The USTC Future Media Computing Lab is run by <a href=/author/xiaojun-chang/>Prof Xiaojun Chang</a> and <a href=/author/xun-yang/>Prof Xun Yang</a>. It is a cutting-edge research center dedicated to advancing the frontiers of multimedia and AI technologies. Focused on areas such as video content analysis, multimodal intelligence, 3D vision, and human-computer interaction, the lab aims to revolutionize how media is processed, understood, and generated. Through interdisciplinary collaboration, the lab develops innovative algorithms and systems that address real-world challenges, from video-based recognition tasks to intelligent media creation, fostering breakthroughs in both academic and industrial applications.</p><p>The USTC Future Media Computing Lab is always looking for talented undergraduate, graduate students and postdocs. If you&rsquo;re interested in working in the exciting field of future media computing, feel free to <a href=/#contact>reach out</a>!</p><div class=row></div></div></div></div></section><section id=people class="home-section wg-people"><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Meet the Team</h1></div><div class=col-md-12><h2 class=mb-4>Researchers</h2></div><div class="col-12 col-sm-auto people-person"><a href=/author/xiaojun-chang/><img class="avatar avatar-circle" src=/author/xiaojun-chang/avatar_hudef30b94a392be4f95107e96ebbfd10d_3770_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/xiaojun-chang/>Xiaojun Chang</a></h2><h3>Chair Professor / Director</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/xun-yang/><img class="avatar avatar-circle" src=/author/xun-yang/avatar_hu5e4fb6c689510e0963f9ffb2bdb6b4e7_2790_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/xun-yang/>Xun Yang</a></h2><h3>Professor / Deputy Director</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/zhihui-li/><img class="avatar avatar-circle" src=/author/zhihui-li/avatar_hucb610fb86b60c80d4910cfbef7b8b79d_3204_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/zhihui-li/>Zhihui Li</a></h2><h3>Professor</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/lei-chen/><img class="avatar avatar-circle" src=/author/lei-chen/avatar_hu604a2fa950eb5d71b107cc4bbc8a1e9b_31398_270x270_fill_q90_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/lei-chen/>Lei Chen</a></h2><h3>Associate Researcher</h3></div></div></div></div></section><section id=featured class="home-section wg-featured"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Featured Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span><a href=/author/guangrun-wang/>Guangrun Wang</a></span>, <span><a href=/author/changlin-li/>Changlin Li</a></span>, <span><a href=/author/liuchun-yuan/>Liuchun Yuan</a></span>, <span><a href=/author/jiefeng-peng/>Jiefeng Peng</a></span>, <span><a href=/author/xiaoyu-xian/>Xiaoyu Xian</a></span>, <span><a href=/author/xiaodan-liang/>Xiaodan Liang</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/liang-lin/>Liang Lin</a></span></div><span class=article-date>May 2024</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/dna_family_boosting_weight-sharing_nas_with_block-wise_supervisions/>DNA Family: Boosting Weight-Sharing NAS With Block-Wise Supervisions</a></h3><a href=/publication/dna_family_boosting_weight-sharing_nas_with_block-wise_supervisions/ class=summary-link><div class=article-style><p>This paper presents the DNA Family, a new framework for boosting the effectiveness of weight-sharing Neural Architecture Search (NAS) by dividing large search spaces into smaller blocks and applying block-wise supervisions. The approach demonstrates high performance on benchmarks such as ImageNet, surpassing previous NAS techniques in accuracy and efficiency.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/10324326 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/dna_family_boosting_weight-sharing_nas_with_block-wise_supervisions/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2023.3335261 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/changlin-li/>Changlin Li</a></span>, <span><a href=/author/guangrun-wang/>Guangrun Wang</a></span>, <span><a href=/author/bing-wang/>Bing Wang</a></span>, <span><a href=/author/xiaodan-liang/>Xiaodan Liang</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span></div><span class=article-date>April 2024</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/ds-net_dynamic_weight_slicing_for_efficient_inference_in_cnns_and_vision_transformers/>DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Vision Transformers</a></h3><a href=/publication/ds-net_dynamic_weight_slicing_for_efficient_inference_in_cnns_and_vision_transformers/ class=summary-link><div class=article-style><p>This paper presents DS-Net++, a novel framework for efficient inference in neural networks. Dynamic weight slicing allows for scalable performance across multiple architectures like CNNs and vision transformers. The method delivers up to 61.5% real-world acceleration with minimal accuracy drops on models like MobileNet, ResNet-50, and Vision Transformer, showing its potential in hardware-efficient dynamic networks.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9842348 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/ds-net_dynamic_weight_slicing_for_efficient_inference_in_cnns_and_vision_transformers/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2022.3194044 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/caixia-yan/>Caixia Yan</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/minnan-luo/>Minnan Luo</a></span>, <span><a href=/author/huan-liu/>Huan Liu</a></span>, <span><a href=/author/xiaoqin-zhang/>Xiaoqin Zhang</a></span>, <span><a href=/author/qinghua-zheng/>Qinghua Zheng</a></span></div><span class=article-date>March 2024</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/semantics-guided_contrastive_network_for_zero-shot_object_detection/>Semantics-Guided Contrastive Network for Zero-Shot Object Detection</a></h3><a href=/publication/semantics-guided_contrastive_network_for_zero-shot_object_detection/ class=summary-link><div class=article-style><p>This paper presents ContrastZSD, a semantics-guided contrastive network for zero-shot object detection (ZSD). The framework improves visual-semantic alignment and mitigates the bias problem towards seen classes by incorporating region-category and region-region contrastive learning. ContrastZSD demonstrates superior performance in both ZSD and generalized ZSD tasks across PASCAL VOC and MS COCO datasets.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9669022 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/semantics-guided_contrastive_network_for_zero-shot_object_detection/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2021.3140070 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/lingling-zhang/>Lingling Zhang</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/jun-liu/>Jun Liu</a></span>, <span><a href=/author/minnan-luo/>Minnan Luo</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/lina-yao/>Lina Yao</a></span>, <span><a href=/author/alex-hauptmann/>Alex Hauptmann</a></span></div><span class=article-date>March 2024</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/tn-zstad_transferable_network_for_zero-shot_temporal_activity_detection/>TN-ZSTAD: Transferable Network for Zero-Shot Temporal Activity Detection</a></h3><a href=/publication/tn-zstad_transferable_network_for_zero-shot_temporal_activity_detection/ class=summary-link><div class=article-style><p>TN-ZSTAD introduces a novel approach to zero-shot temporal activity detection (ZSTAD) in long untrimmed videos. By integrating an activity graph transformer with zero-shot detection techniques, it addresses the challenge of recognizing and localizing unseen activities. Experiments on THUMOS'14, Charades, and ActivityNet datasets validate its superior performance in detecting unseen activities.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9797852 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/tn-zstad_transferable_network_for_zero-shot_temporal_activity_detection/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2022.3183586 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/zhe-liu/>Zhe Liu</a></span>, <span><a href=/author/yun-li/>Yun Li</a></span>, <span><a href=/author/lina-yao/>Lina Yao</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/wei-fang/>Wei Fang</a></span>, <span><a href=/author/xiaojun-wu/>Xiaojun Wu</a></span>, <span><a href=/author/abdulmotaleb-el-saddik/>Abdulmotaleb El Saddik</a></span></div><span class=article-date>January 2024</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/simple_primitives_with_feasibility-_and_contextuality-dependence_for_open-world_compositional_zero-shot_learning/>Simple Primitives With Feasibility- and Contextuality-Dependence for Open-World Compositional Zero-Shot Learning</a></h3><a href=/publication/simple_primitives_with_feasibility-_and_contextuality-dependence_for_open-world_compositional_zero-shot_learning/ class=summary-link><div class=article-style><p>This paper proposes the SAD-SP model, which improves open-world compositional zero-shot learning by capturing contextuality and feasibility dependencies between states and objects. Using semantic attention and knowledge disentanglement, the approach enhances performance on benchmarks like MIT-States and C-GQA by predicting unseen compositions more accurately.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/10274865 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/simple_primitives_with_feasibility-_and_contextuality-dependence_for_open-world_compositional_zero-shot_learning/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2023.3323012 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/haoyu-zhang/>Haoyu Zhang</a></span>, <span><a href=/author/meng-liu/>Meng Liu</a></span>, <span><a href=/author/yuhong-li/>Yuhong Li</a></span>, <span><a href=/author/ming-yan/>Ming Yan</a></span>, <span><a href=/author/zan-gao/>Zan Gao</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/liqiang-nie/>Liqiang Nie</a></span></div><span class=article-date>December 2023</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/attribute-guided_collaborative_learning_for_partial_person_re-identification/>Attribute-Guided Collaborative Learning for Partial Person Re-Identification</a></h3><a href=/publication/attribute-guided_collaborative_learning_for_partial_person_re-identification/ class=summary-link><div class=article-style><p>This paper introduces a novel framework for partial person re-identification, addressing the challenge of image spatial misalignment due to occlusions. The framework utilizes an adaptive threshold-guided masked graph convolutional network and incorporates human attributes to enhance the accuracy of pedestrian representations. Experimental results demonstrate its effectiveness across multiple public datasets.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/10239469 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/attribute-guided_collaborative_learning_for_partial_person_re-identification/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2023.3312302 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/caixia-yan/>Caixia Yan</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/weili-guan/>Weili Guan</a></span>, <span><a href=/author/zongyuan-ge/>Zongyuan Ge</a></span>, <span><a href=/author/lei-zhu/>Lei Zhu</a></span>, <span><a href=/author/qinghua-zheng/>Qinghua Zheng</a></span></div><span class=article-date>December 2023</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/zeronas_differentiable_generative_adversarial_networks_search_for_zero-shot_learning/>ZeroNAS: Differentiable Generative Adversarial Networks Search for Zero-Shot Learning</a></h3><a href=/publication/zeronas_differentiable_generative_adversarial_networks_search_for_zero-shot_learning/ class=summary-link><div class=article-style><p>ZeroNAS presents a differentiable generative adversarial network architecture search method specifically designed for zero-shot learning (ZSL). The approach optimizes both generator and discriminator architectures, leading to significant improvements in ZSL and generalized ZSL tasks across various datasets.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9612044 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/zeronas_differentiable_generative_adversarial_networks_search_for_zero-shot_learning/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2021.3127346 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/pengfei-xu/>Pengfei Xu</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/luyao-yang/>Luyao Yang</a></span>, <span><a href=/author/yuanyuan-zhang/>Yuanyuan Zhang</a></span>, <span><a href=/author/lina-yao/>Lina Yao</a></span>, <span><a href=/author/xiaojiang-chen/>Xiaojiang Chen</a></span></div><span class=article-date>August 2023</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/when_object_detection_meets_knowledge_distillation_a_survey/>When Object Detection Meets Knowledge Distillation: A Survey</a></h3><a href=/publication/when_object_detection_meets_knowledge_distillation_a_survey/ class=summary-link><div class=article-style><p>This paper provides a comprehensive review of the recent advancements in knowledge distillation (KD)-based object detection (OD) models. It covers different KD strategies for improving object detection tasks, such as incremental OD, small object detection, and weakly supervised OD. The paper also explores advanced distillation techniques and highlights future research directions in the field.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/10070820 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/when_object_detection_meets_knowledge_distillation_a_survey/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2023.3257546 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/mingjie-li/>Mingjie Li</a></span>, <span><a href=/author/po-yao-huang/>Po-Yao Huang</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/junjie-hu/>Junjie Hu</a></span>, <span><a href=/author/yi-yang/>Yi Yang</a></span>, <span><a href=/author/alex-hauptmann/>Alex Hauptmann</a></span></div><span class=article-date>March 2023</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/video_pivoting_unsupervised_multi-modal_machine_translation/>Video Pivoting Unsupervised Multi-Modal Machine Translation</a></h3><a href=/publication/video_pivoting_unsupervised_multi-modal_machine_translation/ class=summary-link><div class=article-style><p>This paper introduces a video pivoting method for unsupervised multi-modal machine translation (UMMT), which uses spatial-temporal graphs to align sentence pairs in the latent space. By leveraging visual content from videos, the approach enhances translation accuracy and generalization across multiple languages, as demonstrated on the VATEX and HowToWorld datasets.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9792411 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/video_pivoting_unsupervised_multi-modal_machine_translation/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2022.3181116 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/pengzhen-ren/>Pengzhen Ren</a></span>, <span><a href=/author/pengfei-xu/>Pengfei Xu</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/xiaojiang-chen/>Xiaojiang Chen</a></span>, <span><a href=/author/alex-hauptmann/>Alex Hauptmann</a></span></div><span class=article-date>January 2023</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/a_comprehensive_survey_of_scene_graphs_generation_and_application/>A Comprehensive Survey of Scene Graphs: Generation and Application</a></h3><a href=/publication/a_comprehensive_survey_of_scene_graphs_generation_and_application/ class=summary-link><div class=article-style><p>This survey provides a thorough exploration of the concept of scene graphs, discussing their role in visual understanding tasks. Scene graphs represent objects, their attributes, and relationships, helping improve tasks like visual reasoning and image captioning. The paper outlines various generation methods and applications, and also highlights key challenges like the long-tailed distribution of relationships.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9661322 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/a_comprehensive_survey_of_scene_graphs_generation_and_application/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2021.3137605 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/miao-zhang/>Miao Zhang</a></span>, <span><a href=/author/huiqi-li/>Huiqi Li</a></span>, <span><a href=/author/shirui-pan/>Shirui Pan</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/chuan-zhou/>Chuan Zhou</a></span>, <span><a href=/author/zongyuan-ge/>Zongyuan Ge</a></span>, <span><a href=/author/steven-su/>Steven Su</a></span></div><span class=article-date>September 2021</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/one-shot_neural_architecture_search_maximising_diversity_to_overcome_catastrophic_forgetting/>One-Shot Neural Architecture Search: Maximising Diversity to Overcome Catastrophic Forgetting</a></h3><a href=/publication/one-shot_neural_architecture_search_maximising_diversity_to_overcome_catastrophic_forgetting/ class=summary-link><div class=article-style><p>This work addresses the catastrophic forgetting problem in one-shot neural architecture search by treating supernet training as a constrained optimization problem. The proposed method uses a novelty search-based architecture selection approach to enhance diversity and boost performance, achieving competitive results on CIFAR-10 and ImageNet datasets.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9247292 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/one-shot_neural_architecture_search_maximising_diversity_to_overcome_catastrophic_forgetting/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2020.3035351 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/jianfeng-dong/>Jianfeng Dong</a></span>, <span><a href=/author/xirong-li/>Xirong Li</a></span>, <span><a href=/author/chaoxi-xu/>Chaoxi Xu</a></span>, <span><a href=/author/xun-yang/>Xun Yang</a></span>, <span><a href=/author/gang-yang/>Gang Yang</a></span>, <span><a href=/author/xun-wang/>Xun Wang</a></span></div><span class=article-date>February 2021</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/dual_encoding_video_retrieval_by_text/>Dual Encoding for Video Retrieval by Text</a></h3><a href=/publication/dual_encoding_video_retrieval_by_text/ class=summary-link><div class=article-style><p>This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described exclusively in the form of a natural-language sentence, with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is crucial. To that end, the two modalities need to be first encoded into real-valued vectors and then projected into a common space. In this paper we achieve this by proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Our novelty is two-fold. First, different from prior art that resorts to a specific single-level encoder, the proposed network performs multi-level encoding that represents the rich content of both modalities in a coarse-to-fine fashion. Second, different from a conventional common space learning algorithm which is either concept based or latent space based, we introduce hybrid space learning which combines the high performance of the latent space and the good interpretability of the concept space. Dual encoding is conceptually simple, practically effective and end-to-end trained with hybrid space learning. Extensive experiments on four challenging video datasets show the viability of the new method. Code and data are available at <a href=https://github.com/danieljf24/hybrid_space target=_blank rel=noopener>https://github.com/danieljf24/hybrid_space</a></p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/abstract/document/9354593/ target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/dual_encoding_video_retrieval_by_text/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2021.3059295 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/yao-liang-yu/>Yao-Liang Yu</a></span>, <span><a href=/author/yi-yang/>Yi Yang</a></span>, <span><a href=/author/eric-p.-xing/>Eric P. Xing</a></span></div><span class=article-date>August 2017</span>
<span class=middot-divider></span>
<span class=pub-publication>IEEE Transactions on Pattern Analysis and Machine Intelligence</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/semantic_pooling_for_complex_event_analysis_in_untrimmed_videos/>Semantic Pooling for Complex Event Analysis in Untrimmed Videos</a></h3><a href=/publication/semantic_pooling_for_complex_event_analysis_in_untrimmed_videos/ class=summary-link><div class=article-style><p>This paper introduces a novel semantic pooling method for event analysis tasks like detection, recognition, and recounting in long untrimmed Internet videos. Using semantic saliency, the approach ranks video shots to prioritize the most relevant ones, improving the classifier’s accuracy. The paper proposes a nearly-isotonic SVM classifier, validated with experiments on real-world datasets, showcasing significant performance improvements.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/7565615 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/semantic_pooling_for_complex_event_analysis_in_untrimmed_videos/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2016.2608901 target=_blank rel=noopener>DOI</a></div></div></div></div></div></section><section id=publications class="home-section wg-pages"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class="alert alert-note"><div>Quickly discover relevant content by <a href=/publication/>filtering publications</a>.</div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/dna_family_boosting_weight-sharing_nas_with_block-wise_supervisions/>DNA Family: Boosting Weight-Sharing NAS With Block-Wise Supervisions</a></h3><a href=/publication/dna_family_boosting_weight-sharing_nas_with_block-wise_supervisions/ class=summary-link><div class=article-style>This paper presents the DNA Family, a new framework for boosting the effectiveness of weight-sharing Neural Architecture Search (NAS) …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/guangrun-wang/>Guangrun Wang</a></span>, <span><a href=/author/changlin-li/>Changlin Li</a></span>, <span><a href=/author/liuchun-yuan/>Liuchun Yuan</a></span>, <span><a href=/author/jiefeng-peng/>Jiefeng Peng</a></span>, <span><a href=/author/xiaoyu-xian/>Xiaoyu Xian</a></span>, <span><a href=/author/xiaodan-liang/>Xiaodan Liang</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/liang-lin/>Liang Lin</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/10324326 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/dna_family_boosting_weight-sharing_nas_with_block-wise_supervisions/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2023.3335261 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/ds-net_dynamic_weight_slicing_for_efficient_inference_in_cnns_and_vision_transformers/>DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Vision Transformers</a></h3><a href=/publication/ds-net_dynamic_weight_slicing_for_efficient_inference_in_cnns_and_vision_transformers/ class=summary-link><div class=article-style>This paper presents DS-Net++, a novel framework for efficient inference in neural networks. Dynamic weight slicing allows for scalable …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/changlin-li/>Changlin Li</a></span>, <span><a href=/author/guangrun-wang/>Guangrun Wang</a></span>, <span><a href=/author/bing-wang/>Bing Wang</a></span>, <span><a href=/author/xiaodan-liang/>Xiaodan Liang</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9842348 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/ds-net_dynamic_weight_slicing_for_efficient_inference_in_cnns_and_vision_transformers/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2022.3194044 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/semantics-guided_contrastive_network_for_zero-shot_object_detection/>Semantics-Guided Contrastive Network for Zero-Shot Object Detection</a></h3><a href=/publication/semantics-guided_contrastive_network_for_zero-shot_object_detection/ class=summary-link><div class=article-style>This paper presents ContrastZSD, a semantics-guided contrastive network for zero-shot object detection (ZSD). The framework improves …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/caixia-yan/>Caixia Yan</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/minnan-luo/>Minnan Luo</a></span>, <span><a href=/author/huan-liu/>Huan Liu</a></span>, <span><a href=/author/xiaoqin-zhang/>Xiaoqin Zhang</a></span>, <span><a href=/author/qinghua-zheng/>Qinghua Zheng</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9669022 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/semantics-guided_contrastive_network_for_zero-shot_object_detection/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2021.3140070 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/tn-zstad_transferable_network_for_zero-shot_temporal_activity_detection/>TN-ZSTAD: Transferable Network for Zero-Shot Temporal Activity Detection</a></h3><a href=/publication/tn-zstad_transferable_network_for_zero-shot_temporal_activity_detection/ class=summary-link><div class=article-style>TN-ZSTAD introduces a novel approach to zero-shot temporal activity detection (ZSTAD) in long untrimmed videos. By integrating an …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/lingling-zhang/>Lingling Zhang</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/jun-liu/>Jun Liu</a></span>, <span><a href=/author/minnan-luo/>Minnan Luo</a></span>, <span><a href=/author/zhihui-li/>Zhihui Li</a></span>, <span><a href=/author/lina-yao/>Lina Yao</a></span>, <span><a href=/author/alex-hauptmann/>Alex Hauptmann</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9797852 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/tn-zstad_transferable_network_for_zero-shot_temporal_activity_detection/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2022.3183586 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=/publication/simple_primitives_with_feasibility-_and_contextuality-dependence_for_open-world_compositional_zero-shot_learning/>Simple Primitives With Feasibility- and Contextuality-Dependence for Open-World Compositional Zero-Shot Learning</a></h3><a href=/publication/simple_primitives_with_feasibility-_and_contextuality-dependence_for_open-world_compositional_zero-shot_learning/ class=summary-link><div class=article-style>This paper proposes the SAD-SP model, which improves open-world compositional zero-shot learning by capturing contextuality and …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/zhe-liu/>Zhe Liu</a></span>, <span><a href=/author/yun-li/>Yun Li</a></span>, <span><a href=/author/lina-yao/>Lina Yao</a></span>, <span><a href=/author/xiaojun-chang/>Xiaojun Chang</a></span>, <span><a href=/author/wei-fang/>Wei Fang</a></span>, <span><a href=/author/xiaojun-wu/>Xiaojun Wu</a></span>, <span><a href=/author/abdulmotaleb-el-saddik/>Abdulmotaleb El Saddik</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/10274865 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/simple_primitives_with_feasibility-_and_contextuality-dependence_for_open-world_compositional_zero-shot_learning/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TPAMI.2023.3323012 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=tags class="home-section wg-tag-cloud"><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Popular Topics</h1></div><div class="col-12 col-lg-8"><div class=tag-cloud><a href=/tag/active-learning/ style=font-size:.7rem>Active learning</a>
<a href=/tag/activity-graph-transformer/ style=font-size:.7rem>Activity Graph Transformer</a>
<a href=/tag/autodl/ style=font-size:.7rem>AutoDL</a>
<a href=/tag/block-wise-learning/ style=font-size:.7rem>Block-wise Learning</a>
<a href=/tag/catastrophic-forgetting/ style=font-size:.7rem>Catastrophic Forgetting</a>
<a href=/tag/cnn/ style=font-size:.7rem>CNN</a>
<a href=/tag/compositional-zero-shot-learning/ style=font-size:.7rem>Compositional Zero-Shot Learning</a>
<a href=/tag/contextuality-dependence/ style=font-size:.7rem>Contextuality-Dependence</a>
<a href=/tag/continual-learning/ style=font-size:.7rem>Continual Learning</a>
<a href=/tag/continuous-search-strategy/ style=font-size:.7rem>Continuous search strategy</a>
<a href=/tag/contrastive-learning/ style=font-size:.7rem>Contrastive Learning</a>
<a href=/tag/deep-active-learning/ style=font-size:.7rem>Deep active learning</a>
<a href=/tag/deep-learning/ style=font-size:1.259879525495411rem>Deep Learning</a>
<a href=/tag/dual-encoding/ style=font-size:.7rem>Dual encoding</a>
<a href=/tag/dynamic-inference/ style=font-size:.7rem>Dynamic Inference</a>
<a href=/tag/efficient-inference/ style=font-size:.7rem>Efficient Inference</a>
<a href=/tag/event-detection/ style=font-size:.7rem>Event Detection</a>
<a href=/tag/event-recognition/ style=font-size:.7rem>Event Recognition</a>
<a href=/tag/neural-architecture-search/ style=font-size:1.819759050990822rem>Neural Architecture Search</a>
<a href=/tag/zero-shot-learning/ style=font-size:1.259879525495411rem>Zero-Shot Learning</a></div></div></div></div></section><section id=contact class="home-section wg-contact"><div class=container><div class="row contact-widget"><div class="col-12 col-lg-4 section-heading"><h1>Contact</h1></div><div class="col-12 col-lg-8"><div class=mb-3><form name=contact method=post action=https://formspree.io/xjchang@ustc.edu.cn><div class="form-group form-inline"><label class=sr-only for=inputName>Name</label>
<input type=text name=name class="form-control w-100" id=inputName placeholder=Name required></div><div class="form-group form-inline"><label class=sr-only for=inputEmail>Email</label>
<input type=email name=email class="form-control w-100" id=inputEmail placeholder=Email required></div><div class=form-group><label class=sr-only for=inputMessage>Message</label>
<textarea name=message class=form-control id=inputMessage rows=5 placeholder=Message required></textarea></div><button type=submit class="btn btn-outline-primary px-3 py-2">Send</button></form></div><ul class=fa-ul><li><i class="fa-li fas fa-map-marker fa-2x" aria-hidden=true></i>
<span id=person-address>中国科学技术大学高新校区, 合肥市, 安徽省 230088</span></li><li><i class="fa-li fab fa-github fa-2x" aria-hidden=true></i>
<a href=https://github.com/xiaojun-chang target=_blank rel=noopener>Github Contact</a></li></ul><div class=d-none><input id=map-provider value=2>
<input id=map-lat value=31.820132553621974>
<input id=map-lng value=117.12195038760376>
<input id=map-dir value="中国科学技术大学高新校区, 合肥市, 安徽省 230088">
<input id=map-zoom value=15>
<input id=map-api-key value=pk.eyJ1IjoiZ2VuZXRpY2xvZ2ljbGFiIiwiYSI6ImNraHkxNzhwczAxZ2syeWw2eXk1ajFzd3QifQ.OkfUVvBL_nR39T8TtBZetg></div><div id=map></div></div></div></div></section><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script>
<script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",talk:"Talks",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script>window.netlifyIdentity&&window.netlifyIdentity.on("init",e=>{e||window.netlifyIdentity.on("login",()=>{document.location.href="/admin/"})})</script><script src=/js/wowchemy.min.ee9dd1b51c1cbfc9adeb51e02586011c.js></script><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href=https://wowchemy.com target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>